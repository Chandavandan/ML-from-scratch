{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fea9c45a-40de-4cd5-8802-54db3d1747f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch GD\n",
    "class GDregressor:\n",
    "    def __init__(self, learning_rate=0.01, epochs=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train).reshape(-1,1)\n",
    "\n",
    "        m, n = X_train.shape\n",
    "\n",
    "        # initialize\n",
    "        self.intercept_ = 0.0\n",
    "        self.coef_ = np.ones((n,1))\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "\n",
    "            # prediction\n",
    "            # vectorization\n",
    "            y_hat = (X_train @ self.coef_) + self.intercept_\n",
    "\n",
    "            # error\n",
    "            error = y_train - y_hat\n",
    "\n",
    "            # gradients\n",
    "            d_b = -2 * np.mean(error)\n",
    "            d_w = -2 * (X_train.T @ error) / m\n",
    "\n",
    "            # update\n",
    "            self.intercept_ -= self.learning_rate * d_b\n",
    "            self.coef_ -= self.learning_rate * d_w\n",
    "\n",
    "            loss = np.mean(error**2)\n",
    "\n",
    "            if _ % 300 == 0:\n",
    "                print(\"Epoch:\", _, \"Loss:\", loss)\n",
    "\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_test = np.array(X_test)\n",
    "        return X_test @ self.coef_ + self.intercept_\n",
    "\n",
    "    def mse(self, y_true, y_pred):\n",
    "        return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "    def r2_score(self,y_true, y_pred):\n",
    "        y_true = np.array(y_true).reshape(-1,1)\n",
    "        y_pred = np.array(y_pred).reshape(-1,1)\n",
    "\n",
    "        ss_res = np.sum((y_true - y_pred) ** 2)   # residual sum of squares\n",
    "        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)  # total sum of squares\n",
    "\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "        return r2\n",
    "\n",
    "# Stochastic GD\n",
    "class SGDregressor:\n",
    "    def __init__(self, learning_rate=0.01, epochs=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train).reshape(-1,1)\n",
    "\n",
    "        m, n = X_train.shape\n",
    "\n",
    "        # initialize\n",
    "        self.intercept_ = 0.0\n",
    "        self.coef_ = np.ones((n,1))\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(X_train.shape[0]):\n",
    "                idx = np.random.randint(0, X_train.shape[0])\n",
    "\n",
    "                xi = X_train[idx].reshape(1, -1)\n",
    "                yi = y_train[idx]\n",
    "\n",
    "                # prediction (single sample)\n",
    "                # vectorization\n",
    "                y_hat = (xi @ self.coef_) + self.intercept_\n",
    "    \n",
    "                # error (single sample)\n",
    "                error = yi - y_hat\n",
    "    \n",
    "                # gradients (single sample)\n",
    "                d_b = -2 * error\n",
    "                d_w = -2 * (xi.T @ error)\n",
    "    \n",
    "                # update immediately\n",
    "                self.intercept_ -= self.learning_rate * d_b\n",
    "                self.coef_ -= self.learning_rate * d_w\n",
    "    \n",
    "    \n",
    "            # monitoring loss \n",
    "            if epoch % 200 == 0:\n",
    "                y_full_pred = X_train @ self.coef_ + self.intercept_\n",
    "                loss = np.mean((y_train - y_full_pred)**2)\n",
    "                print(\"Epoch:\", epoch, \"Loss:\", loss)\n",
    "\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_test = np.array(X_test)\n",
    "        return X_test @ self.coef_ + self.intercept_\n",
    "\n",
    "    def mse(self, y_true, y_pred):\n",
    "        return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "    def r2_score(self,y_true, y_pred):\n",
    "        y_true = np.array(y_true).reshape(-1,1)\n",
    "        y_pred = np.array(y_pred).reshape(-1,1)\n",
    "\n",
    "        ss_res = np.sum((y_true - y_pred) ** 2)   # residual sum of squares\n",
    "        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)  # total sum of squares\n",
    "\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "        return r2\n",
    "\n",
    "# Mini Batch GD\n",
    "class MiniBatchGDregressor:\n",
    "    def __init__(self, batch_size, learning_rate=0.01, epochs=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train).reshape(-1,1)\n",
    "\n",
    "        m, n = X_train.shape\n",
    "        batch = m // self.batch_size\n",
    "\n",
    "        # initialize\n",
    "        self.intercept_ = 0.0\n",
    "        self.coef_ = np.ones((n,1))\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(batch):\n",
    "                idx = np.random.choice(m, self.batch_size, replace=False)\n",
    "\n",
    "                xi = X_train[idx]\n",
    "                yi = y_train[idx]\n",
    "\n",
    "                # prediction \n",
    "                # vectorization\n",
    "                y_hat = (xi @ self.coef_) + self.intercept_\n",
    "    \n",
    "                # error \n",
    "                error = yi - y_hat\n",
    "    \n",
    "                # gradients \n",
    "                d_b = -2 * np.mean(error)\n",
    "                d_w = -2 * (xi.T @ error) / self.batch_size\n",
    "    \n",
    "                # update immediately\n",
    "                self.intercept_ -= self.learning_rate * d_b\n",
    "                self.coef_ -= self.learning_rate * d_w\n",
    "    \n",
    "    \n",
    "            # monitoring loss \n",
    "            if epoch % 200 == 0:\n",
    "                y_full_pred = X_train @ self.coef_ + self.intercept_\n",
    "                loss = np.mean((y_train - y_full_pred)**2)\n",
    "                print(\"Epoch:\", epoch, \"Loss:\", loss)\n",
    "\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_test = np.array(X_test)\n",
    "        return X_test @ self.coef_ + self.intercept_\n",
    "\n",
    "    def mse(self, y_true, y_pred):\n",
    "        return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "    def r2_score(self,y_true, y_pred):\n",
    "        y_true = np.array(y_true).reshape(-1,1)\n",
    "        y_pred = np.array(y_pred).reshape(-1,1)\n",
    "\n",
    "        ss_res = np.sum((y_true - y_pred) ** 2)   # residual sum of squares\n",
    "        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)  # total sum of squares\n",
    "\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "        return r2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7f3a83b2-0e05-4160-aea6-eb11e995cda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# function to generate a raw data\n",
    "def generate_lr_data(\n",
    "    n_samples=200,\n",
    "    n_features=1,\n",
    "    noise_std=1.0,\n",
    "    weight_range=(-5, 5),\n",
    "    bias_range=(-3, 3),\n",
    "    x_range=(-10, 10),\n",
    "    add_outliers=False,\n",
    "    outlier_ratio=0.05,\n",
    "    outlier_strength=15,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate synthetic Linear Regression data:\n",
    "        y = Xw + b + noise\n",
    "\n",
    "    Returns:\n",
    "        X : (n_samples, n_features)\n",
    "        y : (n_samples, 1)\n",
    "        true_w : (n_features, 1)\n",
    "        true_b : float\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Features\n",
    "    X = rng.uniform(x_range[0], x_range[1], size=(n_samples, n_features))\n",
    "\n",
    "    # True weights and bias\n",
    "    true_w = rng.uniform(weight_range[0], weight_range[1], size=(n_features, 1))\n",
    "    true_b = rng.uniform(bias_range[0], bias_range[1])\n",
    "\n",
    "    # Perfect line/plane\n",
    "    y_clean = X @ true_w + true_b\n",
    "\n",
    "    # Add Gaussian noise\n",
    "    noise = rng.normal(0, noise_std, size=(n_samples, 1))\n",
    "    y = y_clean + noise\n",
    "\n",
    "    # Optionally add outliers\n",
    "    if add_outliers:\n",
    "        n_outliers = int(n_samples * outlier_ratio)\n",
    "        outlier_indices = rng.choice(n_samples, n_outliers, replace=False)\n",
    "\n",
    "        # Make y much bigger/smaller randomly\n",
    "        y[outlier_indices] += rng.normal(0, outlier_strength, size=(n_outliers, 1))\n",
    "\n",
    "    return X, y, true_w, true_b\n",
    "\n",
    "\n",
    "def train_test_split_manual(X, Y, test_size=0.2, random_state=42):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y).reshape(-1, 1)\n",
    "\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    indices = np.arange(n_samples)\n",
    "    rng.shuffle(indices)\n",
    "\n",
    "    test_count = int(n_samples * test_size)\n",
    "\n",
    "    test_idx = indices[:test_count]\n",
    "    train_idx = indices[test_count:]\n",
    "\n",
    "    return X[train_idx], X[test_idx], Y[train_idx], Y[test_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "29a101b7-7707-429d-a1ee-a7f88d0aa4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True weights: [ 3.29221141  0.94095987 -2.55145253  2.45675001 -4.15519104  2.74417834\n",
      "  0.89985222 -3.89249682 -4.20494437  2.50836333]\n",
      "True bias: -1.242384560976977\n"
     ]
    }
   ],
   "source": [
    "X, y, w_true, b_true = generate_lr_data(\n",
    "    n_samples=300,\n",
    "    n_features=10,\n",
    "    noise_std=1.5\n",
    ")\n",
    "\n",
    "print(\"True weights:\", w_true.ravel())\n",
    "print(\"True bias:\", b_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3b21e146-fc4b-4943-9594-3173b8bd9d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 2950.5457335678684\n",
      "Epoch: 300 Loss: 895.9227510035776\n",
      "Epoch: 600 Loss: 284.0153149452275\n",
      "Epoch: 900 Loss: 94.10280852559185\n",
      "True: [ 3.29221141  0.94095987 -2.55145253  2.45675001 -4.15519104  2.74417834\n",
      "  0.89985222 -3.89249682 -4.20494437  2.50836333] -1.242384560976977\n",
      "Learned: [ 18.46099573   4.97743551 -13.39405846  11.64592431 -20.81560901\n",
      "  12.28338385   3.84527283 -17.71291302 -19.9999052   12.57180844] 3.082166908407099\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split_manual(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "\"\"\"\n",
    "    During training, gradient descent initially diverged due to large feature magnitudes.\n",
    "    This was resolved by applying z-score normalization (mean=0, std=1) to input features, which significantly improved convergence stability.\n",
    "\"\"\"\n",
    "mean = X_train.mean(axis=0)\n",
    "std = X_train.std(axis=0)\n",
    "\n",
    "X_train = (X_train - mean) / std\n",
    "X_test = (X_test - mean) / std\n",
    "\n",
    "\n",
    "\n",
    "gdr = GDregressor(learning_rate=0.001, epochs=1000)\n",
    "gdr.fit(X_train, y_train)\n",
    "\n",
    "print(\"True:\", w_true.ravel(), b_true)\n",
    "print(\"Learned:\", gdr.coef_.ravel(), gdr.intercept_)\n",
    "\n",
    "\n",
    "# My gradient descent converged successfully as indicated by decreasing loss; \n",
    "# coefficient values differ from ground truth due to feature standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "401457a8-cc3a-475f-b09c-d7935e576235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 2918.8928011844487\n",
      "Epoch: 200 Loss: 443.77816944221865\n",
      "Epoch: 400 Loss: 74.8567149698908\n",
      "Epoch: 600 Loss: 14.8797086907648\n",
      "Epoch: 800 Loss: 4.430082253176249\n",
      "Learned: [ 19.67275196   5.44991575 -14.93405384  14.0527605  -24.07134559\n",
      "  15.15189888   4.98979917 -21.42909965 -23.72043843  13.96401134] [[3.52723066]]\n"
     ]
    }
   ],
   "source": [
    "sgdr = SGDregressor(learning_rate=0.00001, epochs=1000)\n",
    "sgdr.fit(X_train, y_train)\n",
    "\n",
    "print(\"Learned:\", sgdr.coef_.ravel(), sgdr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2476c014-799d-4577-8e4c-1cfa888fd8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 2676.487726717172\n",
      "Epoch: 200 Loss: 1.9957800589386727\n",
      "Epoch: 400 Loss: 1.9950972511567073\n",
      "Epoch: 600 Loss: 1.9957522885074355\n",
      "Epoch: 800 Loss: 1.9968757789631004\n",
      "Learned: [ 19.59415269   5.45796486 -15.03821146  14.36902377 -24.3859084\n",
      "  15.45489712   5.10671334 -21.76287649 -24.05845986  14.05925977] 3.557302054736163\n"
     ]
    }
   ],
   "source": [
    "mbgdr = MiniBatchGDregressor(batch_size=10, learning_rate=0.001, epochs=1000)\n",
    "mbgdr.fit(X_train, y_train)\n",
    "\n",
    "print(\"Learned:\", mbgdr.coef_.ravel(), mbgdr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8182a7dd-767c-4ce8-b2a9-2c8a15a8db71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [  7.87507253  43.65022759 116.33122892 -97.61044568  10.98960873\n",
      " -66.55188456  -6.00581183 -75.21054312 -64.08465188  21.85072646\n",
      "  -1.3742951   18.90881861 -20.23757512 -39.14022115  14.44685565\n",
      "  55.04129086 -13.53423353  35.66211641  56.11425419  33.71423522\n",
      "  10.28499852  36.58364661  41.83397078 -55.51472599 -41.52125557\n",
      "  -7.10941503 -48.74700215 -47.45037428 -35.98452663  -0.73011202\n",
      "  62.33264493 -35.53188691 -42.57024925  94.50073939  59.14680544\n",
      " -77.64666077  -2.68067512  22.84375457 -23.00238024  60.35826543\n",
      " -12.10265045  60.0931546   30.15824817  52.50697398 -44.15710453\n",
      " -58.57910068  43.52024377 -43.67416157  53.42677804 107.11559797\n",
      " -77.06189641 -33.91890756  39.61278986  -1.90167891  22.48187406\n",
      "  16.29033505  -4.52981196  26.96054541  25.94135286 -91.45381713]\n"
     ]
    }
   ],
   "source": [
    "y_pred = gdr.predict(X_test)\n",
    "\n",
    "print(\"Predicted:\", y_pred.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b2eeaa70-06ef-45be-83e0-cc991482e986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [   5.27507651   52.6218521   134.06636494 -113.47682491   14.36273915\n",
      "  -78.72958039   -2.12135383  -85.49701391  -77.43893754   26.10915491\n",
      "   -0.89017354   24.86687535  -21.78424449  -44.11614203   10.34345156\n",
      "   65.08919613  -19.91541079   38.70756746   65.19115567   40.72570536\n",
      "   10.13752059   42.20917729   49.50280623  -63.88193217  -54.9503731\n",
      "   -8.43561331  -53.70659603  -58.95443383  -40.45918228   -2.37622168\n",
      "   72.96265173  -45.54746749  -47.37497479  106.83934682   65.27957841\n",
      "  -90.58119313   -2.80380982   23.03666296  -21.92173446   65.59543882\n",
      "  -12.43331415   67.72170434   36.38284347   57.26157082  -50.15653934\n",
      "  -66.18134555   50.01847041  -46.25710508   62.32554234  122.75473067\n",
      "  -88.90460949  -34.08129465   47.7890786     0.34805422   30.12979135\n",
      "   18.6060617    -6.43909762   33.53286359   27.43059393 -106.66747322]\n"
     ]
    }
   ],
   "source": [
    "y_pred_sgdr = sgdr.predict(X_test)\n",
    "\n",
    "print(\"Predicted:\", y_pred_sgdr.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4e61682a-1147-49fd-8f06-82869b98f191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [   4.82959225   53.60171421  135.37379013 -114.90887754   14.59847164\n",
      "  -79.78618171   -1.4292781   -86.20163104  -78.8943219    26.5359844\n",
      "   -1.07189789   25.54186794  -21.8263538   -44.38437716    9.47674972\n",
      "   66.0032016   -20.74163011   38.68720934   65.73897147   41.49739227\n",
      "    9.91395833   42.65755799   50.3005705   -64.33247152  -56.75328935\n",
      "   -8.42209429  -53.95154348  -60.37996864  -40.7870818    -2.79184397\n",
      "   73.86603853  -46.79430713  -47.77535616  107.71207952   65.45146022\n",
      "  -91.7984467    -2.86252581   22.69398081  -21.45438343   65.74138017\n",
      "  -12.40795017   68.30563982   36.85162656   57.36853416  -50.63643166\n",
      "  -66.54050496   50.43403117  -45.9698451    62.8778155   124.01894693\n",
      "  -89.78980115  -33.70056273   48.89505948    0.65936222   31.15973364\n",
      "   18.50975696   -6.73533004   34.32087548   27.42412049 -107.91989061]\n"
     ]
    }
   ],
   "source": [
    "y_pred_mbgdr = mbgdr.predict(X_test)\n",
    "\n",
    "print(\"Predicted:\", y_pred_mbgdr.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "91b245a5-41bc-46e9-a9c1-7654e9c00a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 77.95026515026714\n",
      "R2 Score: 0.9762891612287307\n"
     ]
    }
   ],
   "source": [
    "mse = gdr.mse(y_test, y_pred)\n",
    "r2 = gdr.r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R2 Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3a5e24e8-13ed-4b6f-8eda-49070c410e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2.92362716781586\n",
      "R2 Score: 0.9991106938216342\n"
     ]
    }
   ],
   "source": [
    "mse_sgdr = sgdr.mse(y_test, y_pred_sgdr)\n",
    "r2_sgdr = sgdr.r2_score(y_test, y_pred_sgdr)\n",
    "\n",
    "print(\"MSE:\", mse_sgdr)\n",
    "print(\"R2 Score:\", r2_sgdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9b0a4e1d-b4bd-4005-8bc8-112f9bdff2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2.122180837764743\n",
      "R2 Score: 0.9993544770169708\n"
     ]
    }
   ],
   "source": [
    "mse_mbgdr = mbgdr.mse(y_test, y_pred_mbgdr)\n",
    "r2_mbgdr = mbgdr.r2_score(y_test, y_pred_mbgdr)\n",
    "\n",
    "print(\"MSE:\", mse_mbgdr)\n",
    "print(\"R2 Score:\", r2_mbgdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6e1feb-688b-44b1-8b79-aeb4c9b03223",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
