{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "fea9c45a-40de-4cd5-8802-54db3d1747f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDregressor:\n",
    "    def __init__(self, learning_rate=0.01, epochs=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train).reshape(-1,1)\n",
    "\n",
    "        m, n = X_train.shape\n",
    "\n",
    "        # initialize\n",
    "        self.intercept_ = 0.0\n",
    "        self.coef_ = np.ones((n,1))\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "\n",
    "            # prediction\n",
    "            y_hat = (X_train @ self.coef_) + self.intercept_\n",
    "\n",
    "            # error\n",
    "            error = y_train - y_hat\n",
    "\n",
    "            # gradients\n",
    "            d_b = -2 * np.mean(error)\n",
    "            d_w = -2 * (X_train.T @ error) / m\n",
    "\n",
    "            # update\n",
    "            self.intercept_ -= self.learning_rate * d_b\n",
    "            self.coef_ -= self.learning_rate * d_w\n",
    "\n",
    "            loss = np.mean(error**2)\n",
    "\n",
    "            if _ % 300 == 0:\n",
    "                print(\"Epoch:\", _, \"Loss:\", loss)\n",
    "\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_test = np.array(X_test)\n",
    "        return X_test @ self.coef_ + self.intercept_\n",
    "\n",
    "    def mse(self, y_true, y_pred):\n",
    "        return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "    def r2_score(self,y_true, y_pred):\n",
    "        y_true = np.array(y_true).reshape(-1,1)\n",
    "        y_pred = np.array(y_pred).reshape(-1,1)\n",
    "\n",
    "        ss_res = np.sum((y_true - y_pred) ** 2)   # residual sum of squares\n",
    "        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)  # total sum of squares\n",
    "\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "        return r2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7f3a83b2-0e05-4160-aea6-eb11e995cda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# function to generate a raw data\n",
    "def generate_lr_data(\n",
    "    n_samples=200,\n",
    "    n_features=1,\n",
    "    noise_std=1.0,\n",
    "    weight_range=(-5, 5),\n",
    "    bias_range=(-3, 3),\n",
    "    x_range=(-10, 10),\n",
    "    add_outliers=False,\n",
    "    outlier_ratio=0.05,\n",
    "    outlier_strength=15,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate synthetic Linear Regression data:\n",
    "        y = Xw + b + noise\n",
    "\n",
    "    Returns:\n",
    "        X : (n_samples, n_features)\n",
    "        y : (n_samples, 1)\n",
    "        true_w : (n_features, 1)\n",
    "        true_b : float\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Features\n",
    "    X = rng.uniform(x_range[0], x_range[1], size=(n_samples, n_features))\n",
    "\n",
    "    # True weights and bias\n",
    "    true_w = rng.uniform(weight_range[0], weight_range[1], size=(n_features, 1))\n",
    "    true_b = rng.uniform(bias_range[0], bias_range[1])\n",
    "\n",
    "    # Perfect line/plane\n",
    "    y_clean = X @ true_w + true_b\n",
    "\n",
    "    # Add Gaussian noise\n",
    "    noise = rng.normal(0, noise_std, size=(n_samples, 1))\n",
    "    y = y_clean + noise\n",
    "\n",
    "    # Optionally add outliers\n",
    "    if add_outliers:\n",
    "        n_outliers = int(n_samples * outlier_ratio)\n",
    "        outlier_indices = rng.choice(n_samples, n_outliers, replace=False)\n",
    "\n",
    "        # Make y much bigger/smaller randomly\n",
    "        y[outlier_indices] += rng.normal(0, outlier_strength, size=(n_outliers, 1))\n",
    "\n",
    "    return X, y, true_w, true_b\n",
    "\n",
    "\n",
    "def train_test_split_manual(X, Y, test_size=0.2, random_state=42):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y).reshape(-1, 1)\n",
    "\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    indices = np.arange(n_samples)\n",
    "    rng.shuffle(indices)\n",
    "\n",
    "    test_count = int(n_samples * test_size)\n",
    "\n",
    "    test_idx = indices[:test_count]\n",
    "    train_idx = indices[test_count:]\n",
    "\n",
    "    return X[train_idx], X[test_idx], Y[train_idx], Y[test_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "29a101b7-7707-429d-a1ee-a7f88d0aa4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True weights: [ 3.29221141  0.94095987 -2.55145253  2.45675001 -4.15519104  2.74417834\n",
      "  0.89985222 -3.89249682 -4.20494437  2.50836333]\n",
      "True bias: -1.242384560976977\n"
     ]
    }
   ],
   "source": [
    "X, y, w_true, b_true = generate_lr_data(\n",
    "    n_samples=300,\n",
    "    n_features=10,\n",
    "    noise_std=1.5\n",
    ")\n",
    "\n",
    "print(\"True weights:\", w_true.ravel())\n",
    "print(\"True bias:\", b_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3b21e146-fc4b-4943-9594-3173b8bd9d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 2950.5457335678684\n",
      "Epoch: 300 Loss: 895.9227510035776\n",
      "Epoch: 600 Loss: 284.0153149452275\n",
      "Epoch: 900 Loss: 94.10280852559185\n",
      "True: [ 3.29221141  0.94095987 -2.55145253  2.45675001 -4.15519104  2.74417834\n",
      "  0.89985222 -3.89249682 -4.20494437  2.50836333] -1.242384560976977\n",
      "Learned: [ 18.46099573   4.97743551 -13.39405846  11.64592431 -20.81560901\n",
      "  12.28338385   3.84527283 -17.71291302 -19.9999052   12.57180844] 3.082166908407099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    My gradient descent converged successfully as indicated by decreasing loss; \\n    coefficient values differ from ground truth due to feature standardization.\\n'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split_manual(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "\"\"\"\n",
    "    During training, gradient descent initially diverged due to large feature magnitudes.\n",
    "    This was resolved by applying z-score normalization (mean=0, std=1) to input features, which significantly improved convergence stability.\n",
    "\"\"\"\n",
    "mean = X_train.mean(axis=0)\n",
    "std = X_train.std(axis=0)\n",
    "\n",
    "X_train = (X_train - mean) / std\n",
    "X_test = (X_test - mean) / std\n",
    "\n",
    "\n",
    "\n",
    "gdr = GDregressor(learning_rate=0.001, epochs=1000)\n",
    "gdr.fit(X_train, y_train)\n",
    "\n",
    "print(\"True:\", w_true.ravel(), b_true)\n",
    "print(\"Learned:\", gdr.coef_.ravel(), gdr.intercept_)\n",
    "\n",
    "\"\"\"\n",
    "    My gradient descent converged successfully as indicated by decreasing loss; \n",
    "    coefficient values differ from ground truth due to feature standardization.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8182a7dd-767c-4ce8-b2a9-2c8a15a8db71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [  7.87507253  43.65022759 116.33122892 -97.61044568  10.98960873\n",
      " -66.55188456  -6.00581183 -75.21054312 -64.08465188  21.85072646\n",
      "  -1.3742951   18.90881861 -20.23757512 -39.14022115  14.44685565\n",
      "  55.04129086 -13.53423353  35.66211641  56.11425419  33.71423522\n",
      "  10.28499852  36.58364661  41.83397078 -55.51472599 -41.52125557\n",
      "  -7.10941503 -48.74700215 -47.45037428 -35.98452663  -0.73011202\n",
      "  62.33264493 -35.53188691 -42.57024925  94.50073939  59.14680544\n",
      " -77.64666077  -2.68067512  22.84375457 -23.00238024  60.35826543\n",
      " -12.10265045  60.0931546   30.15824817  52.50697398 -44.15710453\n",
      " -58.57910068  43.52024377 -43.67416157  53.42677804 107.11559797\n",
      " -77.06189641 -33.91890756  39.61278986  -1.90167891  22.48187406\n",
      "  16.29033505  -4.52981196  26.96054541  25.94135286 -91.45381713]\n"
     ]
    }
   ],
   "source": [
    "y_pred = gdr.predict(X_test)\n",
    "\n",
    "print(\"Predicted:\", y_pred.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "91b245a5-41bc-46e9-a9c1-7654e9c00a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 77.95026515026714\n",
      "R2 Score: 0.9762891612287307\n"
     ]
    }
   ],
   "source": [
    "mse = np.mean((y_test - y_pred)**2)\n",
    "r2 = gdr.r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R2 Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5e24e8-13ed-4b6f-8eda-49070c410e95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0a4e1d-b4bd-4005-8bc8-112f9bdff2d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
